# [MapReduce](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)
map (k1,v1) &rarr; list(k2,v2)

reduce (k2,list(v2)) &rarr; list(v2)

combine (k’, v’) → <k’, v’>*
- Mini-reducers that run in memory after the map phase
- Used as an optimization to reduce network traffic
  
partition (k’, number of partitions) → partition for k’
- Often a simple hash of the key, e.g., hash(k’) mod n
- Divides up key space for parallel reduce operations
  
## Fault Tolerance
### Worker Failure
Any map tasks completed by the worker are reset back to their initial idle state, and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to idle and becomes eligible for rescheduling.

### Master Failure
restart

## Component
### combiner
What is the role of the combiner function in Map Reduce? What is its significance?

The combiner function in MapReduce aggregates intermediate key-value pairs locally on the mapper node before sending them over the network to reducers. It reduces network traffic and improves overall performance by minimizing data transfer, enhancing scalability, and reducing the load on reducers during the shuffle phase.

#### In-mapper combining
Fold the functionality of the combiner into the mapper by preserving state across multiple map calls

**Advantages**

- Explicitly control aggregation – combiners are optional 
- Speed

**Disadvantages**
- Explicit memory management required – if associative array grows too big, it will not fit in memory!
- Preserving state across breaks the functional underpinnings of mapreduce leading to potential order-dependent bugs
- Algorithmic behavior may depend on the order in which input key-value pairs are encountered

### partitioner
What is the role of the partitioner function in Map Reduce? What is its significance?

The partitioner function in MapReduce is responsible for distributing the intermediate key-value pairs generated by the mappers to the reducers. Its significance lies in ensuring that all values associated with a particular key end up at the same reducer, facilitating parallel processing and aggregation. By determining which reducer handles each key, it optimizes the workload distribution across the cluster, enhancing efficiency and scalability. Additionally, it helps prevent data skewness by evenly distributing the processing load among reducers, thereby improving overall performance in large-scale data processing tasks.


## Coding on MapReduce
+ Avoid object creation
  + Inherently costly operation
  + Garbage collection
+ Avoid buffering
  + Limited heap size
  + Works for small datasets, but won’t scale!